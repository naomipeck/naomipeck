<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Introduction to Fieldwork: From elicitation to ELAN</title>
    <meta charset="utf-8" />
    <meta name="author" content="Naomi Peck" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
  </head>
  <body>
    <textarea id="source">



class: inverse, center, middle


# Introduction to Fieldwork: From elicitation to ELAN &lt;br&gt;
## Session 2: What is elicitation? &amp;nbsp;

## Naomi Peck &amp;nbsp;

### Albert-Ludwigs-Universität Freiburg &lt;br&gt; 2022-02-11 (updated: 2022-01-29)
&amp;nbsp;

&lt;img src="freiburg-logo.png" height="125px"/&gt;

&lt;!-- insert VJS logo too? figure this out --&gt;

---

class: center, middle, inverse

# What is elicitation?

---

# Observer's Paradox

&gt; #### ...the aim of linguistic research in the community must be to find out how people talk when they are not being systematically observed; yet we can only obtain these data by systematic observation. &lt;br&gt;&lt;br&gt; (Labov 1972:209)

???

Labov, William. 1972. *Sociolinguistic Patterns*. Philadelphia: University of Pennsylvania Press.

--

To get around the observer's paradox, aim to ask questions which prompt unreserved reactions or engage your consultants in the data collection process, so they become collaborators rather than 'lab rats'.

---

# Metadata

Metadata is data about the data you are collecting which will not necessarily become an object of your research. &lt;br&gt;&lt;br&gt;

--

This could include the age and gender of who has been recorded, the location where the recording was made, or the role of a participant.&lt;br&gt;&lt;br&gt;

--

This becomes very important in e.g. sociolinguistics, where demographic factors can become a relevant factor in explanation. And for those who are sociolinguists, this data is not metadata!&lt;br&gt;&lt;br&gt;

--

It is also very important in interactional disciplines, where people regularly take into account situational factors in their explanation of phenomena. Whether an audience is present or not could radically influence how somebody acts.

---

# Metadata

More typical examples of metadata include:

- Identifiers of texts 

- Languages used

- Format of a file

- Description of a file

- Date of creation of a file

- Relations between files (i.e. video and audio belonging to the same recording session)

- Size of a file in bytes

- Equipment used to create a file (e.g. recording device)

---

# Metadata (Himmelmann 2006:14)

.center[
&lt;img src="./himmelmann2012-14.png" width="70%" height="70%" /&gt;
]

???

Himmelmann, Nikolaus P. 2006. Language documentation: What is it and what is it good for? In Jost Gippert, Nikolaus P. Himmelmann and Ulrike Mosel (eds.), *Essentials in Language Documentation*, 1-30. Berlin: Mouton de Gruyter.

---

class: middle

# What kind of metadata should you collect for your research project?

---

class: middle, center, inverse

# Recording

---

class: middle

&gt; #### *Put simply, audio is presently seen as an &lt;mark&gt;inconvenience&lt;/mark&gt; on the way to transcription, annotation, selection or analysis.* &lt;br&gt;&lt;br&gt; (Nathan 2010:267)

???

Nathan, David. 2010. Sound and unsound practices in documentary linguistics: towards an epistemology for audio. In Peter K. Austin (ed.) *Language Documentation and Description, vol 7*, 262-284. London: SOAS.

---

&lt;iframe width="800" height="600" src="https://www.youtube.com/embed/-XIv2ZYeh98" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

???

Sound Speeds. 2017. "Signal to Noise Ratio - Sound Speeds" [YouTube video]. https://www.youtube.com/watch?v=-XIv2ZYeh98

---

# Tips for better recordings

1. Record for your future self listening back to the track, not for the you now!

1. Achieve the best signal (what you want to be heard) to noise (what you don't want to be heard) ratio as possible.

1. Turn off electrical equipment in the area which contributes to underlying noise.

1. Record in areas without wind and use a windscreen (sometimes called a "dead cat"!).

1. Record in areas which will not 'reflect' sound. You can dampen echoes by using lots of textiles!

1. Avoid creating extra noises when recording, such as typing or clicking a pen.

1. Monitor the recording through a pair of headphones throughout the recording or have a separate person fulfil this role.

---

# Tips for better recordings

1. Find better times to record if it is too noisy.

1. Use an external microphone: these provide much better quality audio than in-built microphones. 

1. The shorter the cord connecting the microphone to the recorder, the better.

1. Use an appropriate microphone. Unidirectional microphones (cardioid; shotgun) are good for capturing the audio of single speakers; omni- or bidirectional microphones are good for when multiple people are present.

1. Adjust your microphone placement properly so that any unidirectional microphones are pointed (roughly) at the bottom lip of a speaker. This should avoid capturing pops created by plosives.

1. Aim to keep audio recording gain at maximum -3~-6dB. A steady level of -12dB works well in my experience.

---

# Further Sources

Good, Jeff. 2022. The scope of linguistic data. In *The Open Handbook of Linguistic Data Management*, edited by Andrea L. Berez-Kroeker, Bradley McDonnell, Eve Koller, and Lauren B. Collister, 27-48. doi.org/10.7551/mitpress/12200.003.0007. Cambridge, MA: MIT Press Open.

Himmelmann, Nikolaus P. 1998. Documentary and descriptive linguistics. *Linguistics* 36:161–195.

Himmelmann, Nikolaus P. 2006. Language documentation: What is it and what is it good for? In Jost Gippert, Nikolaus P. Himmelmann and Ulrike Mosel (eds.), *Essentials in Language Documentation*, 1-30. Berlin: Mouton de Gruyter.

Himmelmann, Nikolaus P. 2012. Linguistic data types and the interface between language documentation and description. *Language Documentation and Conservation* 6:187–207.

Labov, William. 1972. *Sociolinguistic Patterns*. Philadelphia: University of Pennsylvania Press.

Nathan, David. 2010. Sound and unsound practices in documentary linguistics: towards an epistemology for audio. In Peter K. Austin (ed.) *Language Documentation and Description, vol 7*, 262-284. London: SOAS.

---

# Further Sources

Schilling, Natalie. 2013. *Sociolinguistic Fieldwork*. Cambridge: Cambridge University Press.

Schütze, Carson T. (1996) 2016. *The Empirical Base of Linguistics: Grammaticality Judgments and Linguistic Methodology*. Berlin: Language Science Press.

Seyfeddinipur, Mandana and Felix Rau. 2020. Keeping it real: Video data in language documentation and language archiving. *Language Documentation and Conservation* 14, 503-519.

Sullivant, Ryan. 2020. Archival description for language documentation collections. *Language Documentation and Conservation* 14, 520-578.

---

class: inverse, center, middle

# Short Break 

<div class="countdown" id="timer_61f58d7b" style="right:34%;bottom:20%;" data-audio="true" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
